Nandita: Perfect. Now that the recording is on, could you please repeat that your consent? That could you repeat that you consent to being recorded.

Participant 3: Sure I consent being recorded for this interview.

Nandita: Thank you. Your identity, and anything you share with us will be kept confidential, and will only be heard or read by the researchers in our study. We have 5 researchers. Please know that this interview is designed to be a conversation, so there are no right wrong answers. You can skip any question or pause the interview at any time. Do you have any questions before we begin?

Participant 3: Nope, good to go.

Nandita: cool. Alright.

Nandita: So we first start with some introductory topics we'll start by asking a few general questions about your work. Can you tell me briefly about what do you do in your job?

Participant 3: Sure. So I run a team, [ORGANIZATION] for privacy engineering. So what that means is, we're kind of the more of the research on or the the design and implement new solutions piece.

Participant 3: So I don't own all of the privacy engineers. At the company that I work at. There are engineering teams within other groups. But my organization, my team kind of is across the entire [COMPANY]. So we

Participant 3: yeah, that's what I do. So everything from kind of AI privacy, enhancing technologies developing kind of engineering standards.

Participant 3: looking at some work around dark patterns and UX.

Participant 3: So pretty much any privacy engineering topic. But yeah, we we are more focused on the

Participant 3: the exploratory stuff and really getting some of those new solutions tested at scale and then handing them over to operational teams to run.

Nandita: Thank you. Could you also define privacy as you work as in the context of work? How is privacy defined for you? 

Participant 3: You are the second person to ask me this week, so I actually already have an answer written down, which is, I need to make a little video for them. So privacy is understanding people's preferences and respecting their rights when it comes to personal data.

Nandita: So that's that is the official like, like, sort of the the scope of what privacy means at work. 

Participant 3: Yeah, that's that's really the sky. It's it's doing the right thing respecting kind of what people want you to do with their data.

Participant 3: You know, from my perspective, I then take that down into the How do we get this done right? If that's the goal, how do we actually get that done on a day to day? But yeah, that's kind of the overall goal is to to understand what people want, right? Because it's contextual, not everybody wants the same thing. And then how do we make sure that we are able to to respect. You know both their legal rights and then other things that they may want us to do or not do with their data.

Nandita: Interesting. When you think about privacy engineering, how would you dis describe the roles in the industry related to privacy engineering?

Participant 3: I've seen this broken down a couple of different ways. I mean, the way that I have my team organized is I have 3 separate functions, 2 of which are staffed right? So I have privacy, engineering and architecture which is very much kind of a technical side of my team. I have a part called privacy enablement, or a privacy engineering enablement.

Participant 3: which is then kind of things like, you know, making sure that we have communication channels building a community within the company. And and really

Participant 3: ye almost being the listening side of W. What are people concerned about? How do they need to to interact with us. And the piece that I'm working on building out is more on privacy experience. So kind of how do we have a joined up more from a UA ux user experience side of things. While there are a lot of there are a lot of individual requirements to do things with data.

Participant 3: If you go and look at all of the parts of our end to end user experience for privacy, it's very fragmented.

Participant 3: So I want to get somebody whose goal is, we have a whole bunch of really talented UX people, but none of them primus privacy is their main focus and none of our privacy people. We have some who have a bit of ux experience, but it's trying to kind of mirror those to sorry. Marry those 2 together.

Participant 3: Within my engineering function I have somebody who's a a [CRYPTOGRAPHER]. So I know that research is kind of part of some people's view of privacy engineering as well.

Participant 3: And then I have a [STAFF MEMBER] who's more looking at the kind of the overall. You know, how do we? How do we build a privacy reference architecture that we can then implement in the organization? And then engineers kind of supporting that work as well.

Nandita: Interesting. So how would you define a privacy engineer?

Participant 3: It's it's a good question I was hiring for a privacy engineer last year.

Participant 3: and I had an argument with my recruiter because I said, I wanna hire a pets engineer. I want somebody who's got experience with privacy enhancing technology. They know what it is. They know how to implement it, you know, to fiddle around with it and the range of resumes that I got for a privacy engineer was

Participant 3: very, very broad.

Participant 3: I had people who said, Oh, you know I've implemented BigId, or that I've you know I've worked on Gdpr. and it's so, I think to me, an engineer is that combination of

Participant 3: somebody who has experience with, particularly for large organizations, building and managing infrastructure in an enterprise kind of context, and somebody who can take a whether it's something off the shelf. Or, in my case, a lot of the stuff that we build is kind of we're we're building it ourselves

Participant 3: and being able to actually get in and and work with some of those existing systems. You know, some of which might be proprietary and build that privacy enhancement into it. So if I'm talking about, it's really a software engineer with a privacy flavor, would be kind of what I would think of as a privacy engineer.

Participant 3: If somebody's just gonna you know, if somebody could name every law inside and out. But if they can't write a line of code, that's a problem

Participant 3: for an inch. Example

Nandita: sounds good. And now we'd like to move on to like the motivation part of the study. We'd like to talk a little bit about your career journey and how how you got to where you are. So maybe we'll start with. How did you become in interested in privacy engineering as a career?

Participant 3: This is my first job where I've been primarily focused on privacy engineering. So this is kind of this was a transition for me. I enjoy doing new things. I've done almost every role you can think of in a privacy program. And I do have more of a technical background. I was a a

Participant 3: in security before, kind of up to through the CISO Level. So I've kind of, I've you know, manage networks, configured things, a a bunch of that kind of stuff, and it just became a natural for me. This role. Was a natural extension of what I'd done before plus part of my motivation was that I'm not directly responsible for compliance

Participant 3: which is kind of I've I've spent a lot of time, you know, even before Gdpr, kind of going through and and really being focused on that. So it was a nice change of change of focus. And you know, II learned a lot kind of, you know, moving into a role where that the engineering stuff is front and center all the time

Nandita: interesting. And so II think you've already covered the second question that I was gonna ask about like mapping the career journey, and how on how you got to hit this current position. So I'll skip that and then we continue talking about motivations. What motivates you to continue pursuing privacy engineering as a profession? We we saw how you came from security to privacy, but what keeps you in privacy.

Participant 3: I think the thing that got me into privacy in the first place, even before the engineering side of things was, and

Participant 3: back into into security. Back in the late nineties was I like being in spaces where

Participant 3: not all of the answers are known.

Participant 3: So for privacy engineering, I mean, some of the stuff we're doing with. Some of the technologies we're implementing. We're literally looking at the state of the art and saying, That's not good enough. We need to do better.

Participant 3: So anytime you get to do those things rather than just be like, oh, I've gotta implement this again. I've gotta implement another firewall. I've gotta do this right. It's really one of the things that I really like about my job is some of the problems that we are seeing

Participant 3: nobody has sold before. And I think that's been consistent with kind of a lot of my privacy experiences. We're really kind of almost.

Participant 3: We are laying the path for others to follow us.

Participant 3: And that's always been motivational to me is to kind of be working on problems where there isn't really a clear answer. And we're kind of working towards that goal together

Nandita: sounds good. So it's more about like doing hard stuff, doing being the first one to do it. Yeah, exactly. Or or if not, the first one be able to go and say, Yeah, we're one of the first people that are doing this so that that is very satisfying to be to be at that forefront of things

Nandita: interesting. Do you have any other like personal goals?

Nandita: And that's the reason that your input privacy, for example, like you care about human rights or something like that. Maybe not so much from that perspective. I mean, I do think that there is.

Participant 3: There's a lot of abuse of data out there. And I think, even if you know, I I'm I'm not say, you know, I'm not gonna change the world with my job. If all the privacy engineers disappeared tomorrow, you know, would would the world really be a much worse place. Probably, you know. Probably not like it was brain surgeons or someone that's, you know, directly helping people. But I do think that we

Participant 3: we often make people think twice about what they're doing.

Participant 3: I would argue that there aren't necessarily always enough of us to really sway some of these decisions, but some of the work that I've been doing in AI the last 6 months or so is

Participant 3: I some of the questions that my team asks that people haven't considered makes me think that we really are getting people to to take that step back. And oh, you know, maybe we should be a little bit thoughtful about some of the stuff we're doing.

Participant 3: So I think that's it's motivating for me. But I wouldn't say that's the the main reason that I do this job. It's that if you think of the diagram about kind of what are you good at, what can you get paid for, you know it's I. I'm good at this, and I can get paid well for it. So that that's also, you know, a a motivation as well. It's interesting to me, but it's also something that I can enjoy being being good at as opposed to, you know. Just

Participant 3: learning something where I'm I'm in the middle of the pack.

Nandita: Okay? So wanna round up this section on motivation by asking a question about the future. So a year from now do you see yourself in the same position, like in the same field? More specifically, what is it that you're doing what you're doing in your current position.

Participant 3: Yeah, I would see myself being in the same the same position. But the it's extremely likely that I will have a bigger team. I'll have more responsibility. I'll be, you know there'll be new challenges that I'm not dealing with today. So I think that, you know, only having been a year in my my current role. That is something where I would say that there's another few years left of continuing to do different stuff.

Participant 3: you know a unless somebody came to me with a really really attractive offer. I can't see myself moving in the short term.

Nandita: Thank you. I think we're moving on to the third part of the interview. Focus more on the responsibilities and skill sets. So talk about more about your day to day responsibilities to start with. Could you give me an idea of what a typical day for you looks like? Just in terms of like your day to day responsibilities.

Participant 3: Sure way, too many meetings, probably the because, I have a global role in a large organization.

Participant 3: A lot of my time is spent on stakeholder management.

Participant 3: So it's both understanding kind of what my team is doing removing roadblocks, helping them kind of. You know, II had a meeting. Stay. One of my team hit a roadblock. I met with the team that's causing the problem. You know, we've got a plan to address that. So a lot of kind of helping my team get to what they need to do

Participant 3: as in any large organization. There's a lot of kind of there's a lot of things going on, whether update meetings and kind of this is our roadmap around over up. So a lot of the time I'm spent either understanding and absorbing kind of the new stuff that's coming in or making sure that I understand what my stakeholders care about and what they're doing

Participant 3: or then working directly to, you know, to fix a problem or to to work on a we've we've got a an off site in a couple of weeks where I'm presenting a couple of sessions. So I need to write the materials for that. So it's, you know, a a range of things. But I would say a lot of what I do is really

Participant 3: influencing, understanding, and influencing.

Nandita: Interesting. What does your employer like? What responsibility does your employer expect you to take on and work? I'm I'm asking this because sometimes

Nandita: what you do is different from what is expected.

Participant 3: Yeah, the one piece that. So I've taken on a lot of responsibility related to AI that wasn't in my original mandate. So that's something where we're we're working through at the moment, like what happens with that but otherwise it's you know, I we have a formal goal setting process like, Go to green my goals with my boss and kind of so it covers that range of things. I talked about earlier, everything from architectural goals through to implementing certain technologies or

Participant 3: providing you know, training or resources, or or you know, some of the the more educating the rest of the organization. So it is a range of those things. But I think that's the goal setting process. And then kind of regular updates is what keeps me aligned with kind of that overall. You know this is what you should be doing.

Nandita: so it seems like there is a a a lot of similarity, but what what the expectations are, and what you do as part of the goal setting process. But at the time of when you were hired like one year ago was, did you know that this is what you would be doing like? Or was the job description, or what was communicated to you.

Participant 3: The the original scope that I was hired to do was a lot narrower than what I do now. I think I have certainly one of the first things I did when I started the job was to go and meet with

Participant 3: 40 or 50 people from across the organization and kind of, you know, some of the leaders of various teams and and the rest of it. And I use that to then develop a roadmap and a plan and and kind of go from there. But what I found out was that a lot of the things that people were concerned about was not what I was hired to do so I had today, and II had the evidence to go and convince my leadership and say, Hi.

Participant 3: I know you wanna do this stuff. We should also do this stuff as well. And kind of get that alignment on. You know. You can add somebody to go and focus on this. You can go, and, you know, get this included in your scope as well. So a lot of it did evolve. Because my team is a brand new team, right? I was the first person hide into it and setting it up from scratch. So I think that does give you you don't necessarily know what you need

apart from somebody to come in and start working that out.

Nandita: I see. Okay. So I, the root causes. Sometimes you like. Don't have the full idea till that discovery exercise is part of part of the work itself.

Participant 3: Yeah, I think when you've got a well, I when you've got a new team, but also it's worth doing that on a regular basis, because, you know, new products come out and things change. So but yeah, going out and keeping those. You know a lot of people who have adjacent goals to mine. But you know, understanding what they're doing. Where can I use what they're doing? Where can what I'm doing? Help them? It helps, you know. Make sure I'm not just going off and building something, and then nobody's gonna use. It

Nandita: sounds good. So in this next question, we're trying to differentiate between what you do at work and the things that you do outside of work in the same space. Are there any additional responsibilities you feel? You are expected to take on in your role such as like to society or others in the organization or just the community privacy engineering community.

Participant 3: I wouldn't say I wouldn't really say expectations. But part of my part of my role and part of my mandate is to raise the profile of

Participant 3: the company in the privacy, space and privacy engineering space both internally and outside of the company. So you know, II am expected to speak at conferences. And I'm expected to do podcasts and kind of, you know all those things that people are like. Oh, you know, your company actually is doing some interesting stuff in that space. So

Participant 3: I don't know that that said that that's always been part of the expectation. So I don't know that it's kind of outside because it's still driving that same goal of, you know, we're we're doing some cool stuff. But we're also letting people know about it. So that ideally, we can attract more people. And you know, people will will think that we're you know, we we're working on things that are important.

Nandita: So I like, even before the present company, II know you were like super active, and speaking at a lot of conferences, and and going to the what they also like part of the job, or this is something like you personally feel like

Participant 3: or something like that. I've been I had been a consultant for probably [many] years. For for the most part, and yeah for for consultant, particularly when you get up into the more senior ranks. You know you you've always got to be selling and you only sell if you're in front of people. And you're you know, people think, Oh, yeah, I should work with [PARTICIPANT NAME]. So a lot of that kind of

Participant 3: almost just that visibility has been part of my job for a long time. I mean, even back. I moved to the States [many] years ago now. And II mean, I was presenting at international conferences even back then. So yeah, it's it's been part of my job for almost as long as I can remember.

Nandita: Interesting. So we'll talk about skill sets. We talked about the roles. Now we're talking the skill sets and what skills were demanded of you when you started your current role like like, what were the skills that you were hired to do

Participant 3: a lot of it.

Participant 3: I am not the best technical person in the world, and I'd be the first to admit that right my my official title, I think, is director of software engineering. That's my Hr title and I could. I haven't written the line of code in a long time.

Participant 3: So what I was hired to do was more to be able to build a team that could write the code and somebody who was

Participant 3: Had that experience? I think engaging with executives being able to kind of build a consensus to influence people to, you know. Cause when you've got a imagine if you've got a new team that's got no people or one person in it, and you've got all these existing teams.

Participant 3: Sometimes you're gonna get into kind of the scuffles on the edges like, what do you do? That stuff? Is that my stuff? Is that so? Somebody I think that was a big part of it was kind of my breadth of experience with just

Participant 3: helping to manage some of those relationships and that influencing within obviously a strong background in in privacy as well. Right? I could go up credibly against pretty much anybody and say, Well, no, this is why you need to do this because XYZ. So having kind of that, more of the academic privacy background.

Participant 3: and I think then, being able to to innovate and to work out.

Participant 3: You know, what does a privacy engineering [ORGANIZATION] even look like? What does that mean? Right? There weren't very many examples of other coes that are out there. And even when I do look at other companies that do have [ORGANIZATION].

Participant 3: it tends to be more of an overall privacy, [ORGANIZATION], or often something that's engineering specific. So I think, yeah, a lot of that. Yo, you know, demonstrate what it. What do they say in the job? Descriptions like I demonstrate a history of

Participant 3: having, and gone and built new things could have that almost entrepreneurial within a large company.

Nandita: Very interesting. And do you think like these are the same skill sets that you currently use in your job like first versus when you started.

Participant 3: Yeah, I would say that. My, I have certainly brushed up my technical skills in the last year, and especially, I've got some very, very smart people that work for me, so I learn a lot from them and some of the stuff that they're doing. I have a I think I mentioned. I have a [CRYPTOGRAPHER], and she keeps me on my toes with kind of some of the stuff that that she's working on, but also I can help her with like, you know.

Participant 3: this makes sense to me. But you can't go and tell an executive that you've gotta make it, you know, understand? It would lose all the math and the equations. And they, you know, we we do have that the answers. So I think that kind of skill. I I've been strengthening that muscle to really, you know, understand and be able to and talk to a lot of vendors and kind of what are they doing? And you know, getting that real sense of the how do people really engineer these things? But I think all of those other skills that I talked about. I still use every day.

Participant 3: But I've kind of added more of that engineering you know the the day to day, because, you know, if you talk to engineers all day, you end up you know, learning a lot through osmosis.

Nandita: Sure. So what could you like elaborate a little bit more on that. The technical skills you mentioned. And I'm asking this because, there's a lot of ambiguity on

Nandita: what technical skills are needed for, especially like leadership. No, also, if you can give a little bit more context.

Participant 3: sure. So I I'm still not coding but for example, with data quality, I have a much better sense of we have an issue at the moment where one of the things we're doing, we have a data set that doesn't have the right level of data quality. And I can now quantitatively describe. You know, here are the dimensions of data quality that we don't have or we need to go. And you know we can now write. And I probably could write myself. Here are the requirements we need from a data set perspective

Participant 3: that would allow us to then run this tool over this data so I think something like that, or even from a the mathematical underpinnings of trying to prove anonymization. So we we have this

Participant 3: this idea of there are 2 different kinds of anonymization. One is where you are legally required to do it. Let's say you don't have consent to use data, and you can only use it when it's anonymous. That's one that's kind of the hard kind. But then you've got the soft kind, which is more de identification, like we've got a risk scale. And we're doing something to the data to make it safer.

Participant 3: So I think, being able to explain some of that stuff in a way both that can be understood up here. And then also, when we're talking to engineers and saying, Well, you know, we we need to get in and and you know, manipulate the data in this way to get to the result we want?

Participant 3: So I think so. Those are, you know, some of the more technical skills along with

Participant 3: I know a lot more about AI and assessing AI use cases than I did 6 months ago. And that's a lot of research and reading and kind of building checklists and all the rest of it. So I think a lot of that has been more

Participant 3:  more research based, whereas some of the other stuff is more experiential where I'm just in those conversations all the time until the point that you're like, oh, yeah. Now I understand this.

Nandita: I see. So in terms of like, understand the difference between the skills that you interviewed for like when you were like in the process were these sort of the same skills that the questions that were being asked were around the same skill set versus what you're doing on your day job.

Participant 3: I'm

Participant 3: pretty hands on from a management perspective that I want to understand what people are doing, even if not down to the point that I could do their job for them. So if there are people on my team who are doing something I don't want to just say, Well, you're the engineer. You go away and do that until it's done. I want to really understand it. So I think that's the it's maybe being a little bit of a difference between the skills I was hired for or the skills that I had when I was hired. Because

Participant 3: I now have to. You know, II had analysts working for me before, who would say I can take a lord? I could break it down and tell you what needs to be done. So I. You know I learned a lot about that because I had people who are doing that all the time. Now, I've got people who are like, you know, these are some of the problems with

Participant 3: authenticating the data set or creating this encryption algorithm. Or you know. So I think those skills get even though it's not necessarily me doing the work myself. Those skills get enhanced just through. I mean conversations every single day where those kind of topics are being discussed, discussed, discussed.

Nandita: Thank you so much. I know. Super Super, thankful that. You know the Friday evening. And you're talking to us about privacy engineering.

Nandita: Okay, we moving on to the next topic about reporting and deliverables. So wanna understand more about what you're reporting about progress and what sort of deliverables. Are you producing at work? So the first question is, what do you report like? Who do you report to?

Participant 3: So we have. I have at the moment. I have one boss and 2 dotted line bosses so I report formally up through one channel and then informally through a couple of others as well.

Participant 3: So. But we have a a jira structure board that all of our work gets captured in. So we're reporting primarily up through. You know the agile process. And this is the you know, the stories we've closed out this week and all the rest of it. So that's the probably the most detailed version

Participant 3: that will then get rolled up into Qbrs quarterly business reporting and I'll present up to the Vps and the Svps. And kind of you know, this is what we've accomplished for this, you know. One slide, 2 slide kind of level

Participant 3: but I also have reporting out to

Participant 3: to a lot of those internal stakeholders. So I will be presenting at internal conferences, or I'll be going to, you know, other teams, quarterly meetings and saying, Hey, this is what we're doing next quarter, you know. How can we work together? So the reporting is kind of at those couple of different levels. There's the day to day kind of sprint to sprint.

Participant 3: you know, down in the details, reporting that really doesn't go that far outside of my direct management chain but the other reporting is more summarized up to you know, for for an executive level. As to you know, what are you doing? Kind of where are your priorities? How do they align with kind of our overall corporate strategy?

Nandita: I think. The next question is, does anyone report to you? I think you mentioned that there are 2 functions already, and and seem like mentioned, like an engine like an architect. And like a researcher, yeah, I've got an architect to research. I've got a at the moment she reports to me dotted line, but she's more of another

Participant 3: she's got some AI skill sets and some other kind of I wouldn't really so much say an engineer, but kind of she's been in privacy for a long time. And then I also have somebody who's more on the

Participant 3: what I call enablement. So helping to build out the the engineering standards and guidelines, and and more of the less of writing code day to day, but more of still needing to understand privacy and say, I'm now gonna write a standard on de-identification, or I'm gonna write a you know something that we can then communicate out to teams and then use as well.

Participant 3: So yeah, at the moment I have those. What would that be? It's 3, 3 and a half, really. But I do also have another couple of people that we're looking to hire fairly soon. That would bolster, you know, more engineers, and hopefully, somebody who can dedicate themselves to to some of the AI stuff that we're kind of doing, or we're all doing in our spare time right now.

Nandita: So we talked about like how it is working on your current organization. And I was just curious to know if you know about the typical reporting structures. Do you see in the profession, like in the privacy engineering profession, for example. What are the teams and their composition of reporting in in general in the industry?

Participant 3: Yeah, I mean, I've seen engineering teams to report up through business functions. So they're kind of dedicated to, you know, certain product suites and those kind of things.

Participant 3: You know, in in smaller organizations, where I where I was previously

Participant 3: a.

Participant 3: There were no dedicated privacy engineers, even though they were a fortune. 500 company. So I was kind of almost a pseudo, you know. II would deal with a lot of the technical stuff, but not really into the engineering space. So yeah, I've

Participant 3: I think, from talking to other privacy engineers.

Participant 3: It's rare that they seem to report up through the chief privacy officer. It seems to be more common that they're reporting up through, you know, engineering and technical business groups. And I think that seems to be their preference as well. From from what I've heard, but I don't know that I have good data around that

Nandita: interesting and in terms of like the methods you use for reporting you mentioned Jira as rolling up and doing more executive. Are there other other things you need? You use to report, for example, like, if if there's some sort of like

Nandita: like, like recurring emails that are sent out, or some sort of meetings, yeah, I mean we we do have newsletter email newsletters that my team sends out with, kind of what are we doing? There are. We do have office hours for some things

Participant 3: where, you know, we've got a regular slot schedule that if someone's got something to ask us they can dial in and do that. So that's kind of my enablement function. A lot of that is kind of setting up those different channels so that we can be both visible and available to people and help to get that. You know, what are the problems that people are actually concerned about? So yeah, we have those reporting channels as well. In addition to you know, I get asked to speak. We have, like a data science group internally that, I've been asked to speak at before, and some of the stuff we're doing and some of those

Participant 3: you know. II work closely with our our standards office cause we're involved in. Iso standards for both for security and privacy, and sc. 42 for AI

Participant 3: so I work with them. They have conferences as well. It's there's a lot of that kind of like, I say, kind of the stakeholder management piece, just visibility that we exist and who we are. That so? Yeah, that's probably more of the the less formal reporting channels, but more kind of awareness.

Nandita: interesting and would you? What would you say, is the actual organizational structure or meaning like, is it like a relatively flat or a hierarchical organizational structure?

Participant 3:  there certainly is a hierarchy.

Participant 3: I don't know that there's a huge. and

Participant 3: I don't know that there's a huge, very formal kind of structure. So, for example, kind of

Participant 3: under me, and I mean, and even going through people with like, up through 20 years of experience, we have like 4 levels.

Participant 3: So it's pretty, you know, from that perspective, it's and I don't have people at every level in my team. They tend to be on the more senior end of that. So it's fairly flat from that structure, and I know that you know what an expert level is. I could. I know what I expect pretty much across the whole organization.

Participant 3: But when it gets up into the you know. Vp, s, vp, kind of thing that that seems to be a little bit more structured because a a lot of that comes down to the budget, and the head count are assigned to those teams.

Participant 3: Whereas I think for the more junior folks it's more just like, let's all work together to get the problem solved. Say, I, maybe I'm I'm subway kind of right in the middle, which way? So II have visibility into, and a little bit of working in both of those worlds.

Nandita: So in terms of like, what are deliverables are expected from you in this role. For example, you mentioned that? There's a lot of like advisory and like building the processes. What

Nandita: What are these deliverables like? You don't have to like meet the projects, but like high level. What? What does the output of your role look like?

Participant 3: Sure. So yeah, some some documentation, some guidance, that kind of stuff. But also, like we have a a pilot for a pets that we've proved with 10 million records. And now we need to scale it to a hundred 1 million records. So right, that's the next deliverable would be, you know, how do we? How do we do that where we can deliver this in a certain time period. So those kinds of things from more of a technical perspective we had a a deliverable to

Participant 3: be able to calculate re identification risk in certain kinds of data set and actually do that mathematically. So that was a deliverable that we then needed to get approved by legal, so that we can then say, Well, when we've gone through this process we can justify that this is actually anonymous as opposed to

Participant 3: anonymous right where people just throw that word out. And it doesn't really mean anything.

Participant 3: So some of those deliverables are a little bit more like, here's all the math

Participant 3: and we're gonna get a legal approval. And then we can do these things some of them increasingly, are going to be. We have a technology product that can be handed over to an operational team to run.

Participant 3: So we're we're looking at, for example, things like code scanning tools.

Participant 3: You know some of those things where we'll go through and do the assessment and then get to the point where, you know, we actually have completed the deliverable way. We've completed a pilot. We've made a business case. We've identified kind of a home for this thing, and then we'll pass it over to it to

Participant 3: you know, to to run that on an ongoing basis.

Participant 3: So it's a range depending on kind of how going from kind of that more of the procedural side down to the pure tech.

Nandita: This, this, this like a sub question which might be like a little bit of what you've already answered. But can you tell me more about why these deliverables are important for your role?

Participant 3: One of them will save the organization millions of dollars a year if we get it right?

Participant 3: So that's important for my role. Because my leadership is like, you know you, this is a big problem, only you need to help us solve it. Some of the others are more just. How do we help to understand and reduce overall engineering risk. So how do we? Where do we find those pieces of technical debt or privacy debt that we've incurred over the years, you know. How do we get better scope around

Participant 3: just because somebody told us this thing worked this way, does it actually work that way? So yeah, that's some of the business case that we're putting together is like, how do we? How do we really quantify the value of some of the stuff that my team is doing. So it is something that's on my mind with, yeah, is it aligned with a business strategy? Can we show that we've got some risk. We're not managing very well right now that we can help manage.

Participant 3: Is there a manual process that we can automate? Is there? You know other other things we can do to you know. Just show that we're using a a risk reduced version of something rather than a a full on version.

Participant 3: So yeah, that there's a number of different ways of kind of quantifying that but they all really come back to. Can we prove that if somebody said, You know, why, why are you doing this? Why shouldn't we fire? You kind of thing? It's always having that like. This is the value we're providing. And this is kind of how it lines with, you know my, my boss's values there, you know that goals, their goals, their goals all the way up to the top.

Nandita: Do you think that the the deliverables that you described are sort of typical or atypical for someone in this profession, in privacy engineering.

Participant 3: I would say, based on my experience. And I would also say that this kind of extrapolates to security as well a lot of people that are good technologists are not good at engaging with executives.

Participant 3: I think that's kind of my. If if you could say that I have a super power. That's probably it is that I'm really good cause. I've spent so many years doing it making stories very easily consumable for people where privacy is not their day job.

Participant 3: Right? I do a lot of teaching. I do a lot of as you, as you well know, [CONTENT CREATION]. But I think that's the piece where it

Participant 3: I don't know from having met a lot of other privacy engineers that necessarily everybody does that well, but I think a lot of them would probably do the they could build the infrastructure that my team is building. That I can't do right. So II feel that I'm almost in that kind of

Participant 3: the the role. Maybe at the next executive level, up above a lot of these other privacy engineers that I'm I'm familiar with, because, you know, I

Participant 3: I've got a team that does their role for me. And my job is more kind of keeping that air cover, and the alignment and the funding coming, and all the rest of it.

Nandita: So. And how are these deliverables evaluated by your manager.

Participant 3:  With some of the it's easy. It's like we. We actually were able to prove that we could. We could run this pet, and we get the same answer as if we use clear text data. And we could. So some of those we can say, you know it. It's easy with some of the other ones. It's more, you know. What are your contributions? So I mentioned, we've got a an off site meeting in a couple of weeks where somehow I've managed to end up with.

Participant 3: I think of the 4 days like a day and a half I'm responsible for on my team is, and there's a lot of teams going. So I think it's kind of those things where it's you know. What were your contributions, how visible were they? You know, did people feel that they were valuable or useful?

Participant 3: So yeah, some of you know, some of it's soft and some of it's more directly. Quantifiable

Nandita: sounds good we move into the next part of the interview, which is more on challenges and strategies. So we like to now talk about challenges. So the next question is, are there any tools, techniques, or standards that create challenges for you?

Participant 3:  for example. Like, if you're talking about pets or not to get on my soapbox, what are the biggest challenges with pets is that there is very little to no research out there. How do you choose the right one for use? Case I think people go with what they what they know.

Participant 3: But we have had problems where it's like, Oh, could you implement differential privacy? And I'm like

Participant 3: we could. But if you're running massive queries every day you're going to use up the privacy budget in no time, and then it's useless.

Participant 3: right? Because they don't understand thee. So let's not look at that. Let's look at some other solutions like, what are the other solutions that might be on us? So I think that lack of it's not even a standard. It's it's lack of really research or a a common understanding of

Participant 3: how do you go from a business problem to a pet solution? That? We've actually, II think we've done some good research internally, because it didn't exist. But that's been a challenge where I've kind of had to go back and say, this is not the work I want my team to be doing.

Participant 3: but if we don't do it, we're not gonna be able to kind of get to the next stage. So there's been a couple of those things where and even with with AI stuff where we have created an AI assessment, you know, with a whole bunch of inputs. But it's not like you can go. And you know, like you can with pi is. Now you can go and pull one off [ORGANIZATION], or [ORGANIZATION], or whoever else. And they've got all these templates, and it's all well established. And you know that we're we're still, I think, a little bit

Participant 3: lacking in some of those standards. But some of the iso work has been super helpful just to see what's coming kind of coming down the road.

Nandita: Just see that! What are some of the common challenges that you encounter in your job?

Participant 3:  Just getting alignment, getting people on the same page getting people to prioritise some of the work that my team needs to do to meet our goals.

Participant 3: So yeah, I had had a meeting earlier today with someone where she's like, sorry this team didn't get it to you. There's a whole bunch of files going on right now, and it's it's just not their priority. And I'm like. that's okay. My team will stop working on your on your problem until you're ready to prioritize my problem. Then it's so. I think some of that stuff where it's it's not like a hard problem. But we do. We have run into some hard technical problems as well, where

Participant 3: a lot of these solutions that are out there have not been scaled to

Participant 3: petabytes exabytes, you know, hundreds of millions of records a day kind of scale. So we have run into some challenges

Participant 3: and even talking to vendors. I

Participant 3: we we we were talking to a vendor last year where he's like, oh, yeah, you know, we've scaled up to like, you know, one message per device per day at like, you know, a million devices, and I'd like

Participant 3: we don't send one message per day from each device, like when you can get back to us when you can consume thousands of messages per day per 100 million devices. Maybe so. I think some of that scaling particularly right at the very top end. It's really hard as a startup. And and, as you're well aware, a lot of companies in our space are startups that are providing some of these solutions to be able to provide something that operates at our scale.

Participant 3: So that's been a challenge. And I think maybe one that I hadn't fully appreciated when

Participant 3: yeah, some of these some of the vendors are just when we tell them just how much data we have and how much we need. They're like, I don't. I don't know what to do with that.

Nandita: You mentioned the prioritization alignment, and then, in terms of the technical ones, and like actually like building and and evaluating these technologies, especially for scaling. You think that these challenges are either are they typical or atypical for privacy, and, like the profession or the

Participant 3: no, I think they're pretty typical. II mean, you know, not every company is as big as [COMPANY]. But at the same time you've got a lot of even when I was at [COMPANY]. Right? However, how many 1 million you know listings do they have up at any one time?

Participant 3: Yeah, there's a lot of companies that have a lot of scale so I think that piece where you've got the the need for scale. And then a very, you know. Kind of Vc backed. Startup community of of support. We're not even as you know, for for whatever you think about kind of the one trusts of the world.

Participant 3: They've only been around, I mean, II implemented [PRIVACY TOOL] in 2017, and that product was raw. That was really early days like II met their CEO, I think right, as they were launching the product and that was 2017 and a bunch of the stuff that we're looking at now didn't exist 2 years ago. So you know, we we're dealing with kind of a lot. I think a lot of the growing pains in that industry of privacy, engineer, and really coming into its own.

Participant 3: And it also comes back to what I was saying earlier about recruiting where I've got some of these people I'd like. If you're really calling yourself an engineer, then I'm you know, captain of the Titanic kind of thing. It's a you know it really is a stretch

Participant 3: to to say that you're an engineer. If you've implemented big Id once

Nandita: and hopefully, like the premise of doing this this interview, based exercise was to like, truly understand what privacy engineers are doing, to be able to sell. Say that you know this like more defined. This is the definition. And this

Nandita: I think that's where we started with. We wanna do this study. So thanks for for like sharing this insight, it's very helpful. It's been a journey for me over the last year, for sure, and I know you, and I've chatted a couple of times, and it's I've I've always got something new that I've learned, but I do think that the

Participant 3: the relative lack of maturity of the profession is a I'm looking at intern candidates for next summer.

Participant 3: And

Participant 3: the number of people with any kind of privacy, background, or interest is is very low compared to the just. The vast number of computer science

Participant 3: folks that we're seeing. So yeah, it's until it's really, you know, this is a job that I want to do.

Participant 3: You know, as when there's the Lego figure of a privacy engineer. That's what I could retire right in, so it will be an established profession at that point.

Nandita: So moving on to someone, what different challenges we're talking about like technical challenges and alignment challenges. And now now, talking about the organizational and reporting challenges that you face.

Nandita: What would be the most common challenges that you'll encounter in terms of like the just. How it is, how privacy engineer is set up in the organization and how reporting structures work for privacy engineering in your organization. Part of the challenge is, if you want an architectural solution. You've got to convince a bunch of people with who work in different groups with different leadership, with different priorities

Participant 3: to all do something the same way.

Participant 3: So I think that kind of structure. And I've I've talked about this before. The kind of privacy is like this, horizontal in organizations that are structured vertically. And I think that's an ongoing. That's kind of where a lot of my stakeholder management and that stuff comes from. Because, hey, we wanna have one single privacy reference architecture across the whole company. Right? You all need to agree that this is the way we're gonna do it.

Participant 3: You know that then turns into months of kind of meetings and presentations. And

Participant 3: you know it. Not even that. Necessarily. People are arguing over the technical details, but more about like, well, that doesn't align with the way that I do things.

Nandita: So I mean, Do you think that these challenges on alignment and structure are are typical or atypical for privacy engineering as a profession

Participant 3: absolutely typical. I mean, I I've I don't even know the number of companies that I've consulted to over the last, however, many years. But it's funny how often the same problems come up again and again and again. And these are super typical problems not even in super large organizations, but the larger the organization gets, the more kind of alignment that needs to be done to to get anything done at that kind of scale. I think if you're working on, if you're a privacy engineer working on.

Participant 3: let's say you know a feature at Uber.

Participant 3: you're probably going to have much less issue if it's like, you know, I don't need to just this team that's working on this feature when we're good. But I think when you're getting into some of the more architectural pieces, that's when it becomes much more. You've got just so many more people that are impacted.

Participant 3: And some of the stuff that we're looking to do is to replace

Participant 3: systems that were built 5 or 10 years ago with systems that will still work, you know in 10 years time. So there's a lot of investment that's already gone in. You know, you're calling people's babies ugly, effectively, and say, Well, you know, II know you built that, but it doesn't work like it doesn't wait. It barely works now, and it's not gonna work in 5 years. So let's replace it before it breaks.

Participant 3: So some of that stuff as well. It's it's really the like, I said, the the influencing and the the other bits to get people to agree to the technical solution. But I do think that that's it's partially because.

Participant 3: win you as a profession, right?

Participant 3: Even people looking at say, well, we've done a security review. Why do we need to do a privacy review? Right? II still hear that sometimes.

Participant 3: and I think that's pretty common as well.

Nandita: So in terms of like these challenges on on reporting, getting alignment and like of change management at the architecture level, what do you think is the most effective strategies to overcome these challenges, or like what has worked for you

Participant 3: via big consulting firm.

Participant 3: get get, get, get. I mean, I I'm talking cheek because I used to work at a big consulting firm. But it's if you want to convince an executive that you need to go do something hiring an outside consultant is often an effective way of getting that done.

Participant 3: You know, and it's it's

Participant 3: I I've been on both sides of that as well. I mean I laugh. But it is true that sometimes, even if they say exactly the same thing, even if I wrote the report for them, and they put [ORGANIZATION]. On it.

Participant 3: it would sometimes be more believed. So, I think, in the absence of kind of a

Participant 3: and I say 27,001 type adopted standard

Participant 3: because 27, 7 0 one, of course, is pretty new. Not everyone knows about it yet. They get it confused. I think once we have standards, and once we have, you know, people know that they they need to do this stuff right? You look at kind of the sec requirement for cyber security knowledge on the board.

Participant 3: When we get the, you know all of those things help

Participant 3: but it's then really establishing why, you know the more that people already know it when you go to them. And they say, Oh, yeah, I know why this conversation is important. I know what you wanna do? Half the battle is already one, right? I don't feel at the moment that I'm still building that kind of, you know, community or credibility within this company. But if it existed in the rest of the world, and you had other board members saying, we need to spend money on privacy like I now do with cyber, but not when I started in cyber [many] years ago.

Participant 3: right? That has been a huge change in cyber over those years. We'll get there. But I think that's part of the problem is, we don't have that

Participant 3: the the not the broad knowledge within the the executive ranks that this is a thing. And it's really important. And it's not cyber.

Nandita: Yeah, completely resonate. With that. And we're we're at the last section, and we have 5Â min more. So we want to talk about success metrics. So how do you define success in the work that you do?

Participant 3: It's really the did I achieve the outcomes that we're looking for. So you know, I've presented a lot of metrics over the years, and I think the to me the success metrics are always outcome, focused. So it's not so much.

Participant 3: you know. Can we run this cryptographic algorithm over a hundred 1 million data? You know, records in a day. It's more about. You know. How much.

Participant 3: how much money did we save the business by being able to do that? Or what's the you know how many other use cases could we apply that solution to where we've built it once? And then we apply it many. So for me, those are some of the outcomes like, I would love to say, you know, of the top 100 data products that we use across the whole company.

Participant 3: What percentage of those has a privacy, enhanced version of those

Participant 3: right at the moment. I can tell you, it's 0%, because we haven't started doing that work yet. But that's one of the things that's on the goals for this year is to start creeping that number up towards a hundred.

Participant 3: So though those to me are things that are more outcomes. Cause I can say, look now, 80% of the time someone runs a query against our major datasets. They're using pets, and they don't even know it.

Nandita: Interesting. So what would you say? Is the overarching goal like the success metric like? Is there one success metric that

Participant 3: if I could quantify the privacy risk across the company and show that it's less next year, and it's even less the year after. I think that's the metric I'd love to get to. Of course, that's a lot easier said than done. But we can. And we are. We're working on components of that.

Participant 3: But I think even it's finding out the stuff that we don't know right if we go and look at the code and someone's done a review. And they say, Yeah, this looks great. And it turns out the code does something completely different. There's a lot of those kind of blind spots right now, but that's kind of where

Participant 3: you know it. It looks worse. Once you open up the ri, once you lift the rug up until you've actually had a chance to go and clean out. So I would say, year. One of my role was really kind of that foundational, you know, workout who's who work out what their priorities are. And this is the year where we're really like. Okay, now we understand that, let's go and start fixing stuff.

Nandita: So in terms of how do others evaluate the work that you do? So not your reporting change, but but like you know, the peers, and like other logs within

Participant 3: it. It's sad, but it's it's often a lot of what have you done for me lately?

Participant 3: You know. Have you? Have you done anything in the last quarter for marketing. No? Well, why you here then? Well, because I was working on somebody else's problem. That was more, you know. There was more Roi. It was high priority. So I think a lot of that is just it's coming down to you. Gotta meet with people on a regular basis. Understand what they're doing, even if you can't do something for them. Now, is there something else that you can do for them, or tell them that you're, you know. Get to it later. Cause I mean you, you raise a valid point that outside of kind of direct chains

Participant 3: peers could be like I don't know. I mean, I'm sure you're doing useful stuff. But what have you done for me?

Nandita: Do you think there could be some metrics associated with this evaluation, credit right other than like just like

Nandita: like recently, like you did you do something for me like in this quarter? Are there other like more defined metrics which which can help the other

Nandita: teams sort of evaluate impact.

Participant 3: It's a good question. I mean, there's some things where you can say, look, we've we've made the privacy review process 20% faster, right? That impacts you as well. It wasn't a project for you. But it's it's kind of some of the, you know, raising all the boats. Kind of stuff.

Participant 3:  so yeah, I think there are things like that where you you're working on, because we're working on. Of course, a whole bunch of

Participant 3: and global projects and some of those will have a and that's part of the education cause. People may not even see it. They don't even know it's 20% faster. But we've got the metrics to show it.

Nandita: We're going to close this out by asking 2 questions. One is, is there any other thing that you've not mentioned? That you would like to share, or something that you think you we should know as part of this research.

Participant 3: I think an interesting one. And it's interesting for me.

Participant 3: personally is kind of like, where do you go after you've run a privacy engineering team like, what is the next step in the career like, I don't want to necessarily be a chief privacy officer.

Participant 3: But where? Where do you go? Right there? What's the career path? And I think that's important both from kind of the intern. Some people coming in, but really because it's a fairly new profession again, like. So I think, maybe asking about the career path. What would you say? The next step in your career journey. And I would have to say.

Participant 3: I don't really know. That's a question actually, that I have. And I have what's next.

Participant 3: So I think that would be good cause II think it's gonna help. If people can see that the top of the career path is somewhere they wanna be. Maybe it'll encourage more people to to get involved. But yeah, sorry. That's that's the only other thing I can think of. He didn't ask.

Nandita: That's very good question. Actually, thank thank you for bringing that up. And do you have any questions for us?

Participant 3: How how many people are you talking to approximately, and then kind of what are you gonna do with the what's the format gonna be of the of the learnings from this exercise?

Nandita: The format is that there's going to be a report like a research paper that is going to be published, that at like a soups, or like one of the

Nandita: known conferences in terms of how many people we are talking to. We are. We don't have a number in mind. We are. We are going to continue doing these interviews till we reach saturation where there is like no information. Yeah. Yeah. So so it's it's still in the early phases. So it's gonna keep on going. If there are the folks you think we should talk to let us know.

Participant 3: Sounds good.